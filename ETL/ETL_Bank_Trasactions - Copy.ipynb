{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59dba15f-b61d-462b-af4b-179814adbc36",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd227cf0-15df-4dff-b0ed-47927fd5527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob, os, re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bed32b-9220-4d99-87a7-a437783cc1b4",
   "metadata": {},
   "source": [
    "# extract and transform ANZ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b422a03-dd9a-4017-8def-8c167224e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_anz_raw = Path(r'../Data/1.Raw-Bronze/ANZ/')\n",
    "list(folder_path_anz_raw.rglob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e689729-acad-47c9-9a2d-ef711d126dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_anz = []\n",
    "for file in folder_path_anz_raw.rglob('*.csv'):\n",
    "    df_anz = pd.read_csv(file, usecols=['Details','Particulars','Code','Reference','Amount','Date'])[['Date','Details','Particulars','Code','Reference','Amount']]\n",
    "\n",
    "    # adding a new 'File' column to df with file name\n",
    "    df_anz['File'] = file.stem\n",
    "    \n",
    "    # convert to lower case\n",
    "    columns_to_convert = ['Details', 'Particulars', 'Code', 'Reference']\n",
    "    df_anz[columns_to_convert] = df_anz[columns_to_convert].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # merge columns into a new column\n",
    "    columns_to_merge = ['Details', 'Particulars', 'Code', 'Reference']\n",
    "    df_anz['Description'] = df_anz[columns_to_merge].apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "\n",
    "    # convert Date type object to date datatype\n",
    "    # - step 1 - convert series of dates in original 'Date' column from string to datetime object (this is needed for date calculations)\n",
    "    df_anz['Date'] = df_anz['Date'].apply(lambda x:datetime.strptime(x, \"%d/%m/%Y\"))\n",
    "    # - step 2 - re-convert 'Date' datetime column values from datetime object to string object just for presentation purposes - here you can specify any format you like\n",
    "    df_anz['Formatted Date'] = df_anz['Date'].apply(lambda x: x.strftime('%d-%m-%Y-%A'))\n",
    "    \n",
    "    # reorder columns\n",
    "    df_anz = df_anz[['Date','Formatted Date','Description','Details', 'Particulars','Code','Reference','Amount','File']]\n",
    "\n",
    "    # append each dataframe to a list\n",
    "    df_list_anz.append(df_anz)\n",
    "   \n",
    "#df_list\n",
    "\n",
    "final_df_anz = pd.concat(df_list_anz)\n",
    "final_df_anz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22fed9-a474-4bfe-b22e-9ef31fb52fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_anz.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ecdbc-d700-414b-ac6c-acbcb6b0dc57",
   "metadata": {},
   "source": [
    "### load ANZ data to \"Processed-Silver\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81821f8-55b6-4017-bb55-2663349992f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_anz_procs = Path(r'../Data/2.Processed-Silver/')\n",
    "file_anz_processed = folder_path_anz_procs / \"ANZ_Transactions.csv\"\n",
    "final_df_anz.to_csv(file_anz_processed, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37665763-f594-4f61-8239-4736e2d3a5cb",
   "metadata": {},
   "source": [
    "# extract and transform ASB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39237853-ad06-4fff-93a5-e3ff378e3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_asb_raw = Path(r'../Data/1.Raw-Bronze/ASB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8124a0-5c2d-479f-a0d5-a16c7620865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(folder_path_asb_raw.rglob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9ab33-515c-4666-8932-df806f181893",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_asb = []\n",
    "for file in folder_path_asb_raw.rglob('*.csv'):\n",
    "    df_asb = pd.read_csv(file, usecols=['Date','Payee','Memo','Amount'], skiprows=6)[['Date','Payee','Memo','Amount']]\n",
    "    # adding a new 'File' column to df with file name\n",
    "    df_asb['File'] = file.stem\n",
    "    \n",
    "    # convert to lower case\n",
    "    columns_to_convert = ['Payee', 'Memo']\n",
    "    df_asb[columns_to_convert] = df_asb[columns_to_convert].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # merge certain columns into a new column\n",
    "    columns_to_merge = ['Payee', 'Memo']\n",
    "    df_asb['Description'] = df_asb[columns_to_merge].apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "\n",
    "    # convert Date type object to date datatype\n",
    "    # - step 1 - convert series of dates in original 'Date' column from string to datetime object (this is needed for date calculations)\n",
    "    df_asb['Date'] = df_asb['Date'].apply(lambda x:datetime.strptime(x, \"%Y/%m/%d\"))\n",
    "    # - step 2 - re-convert 'Date' datetime column values from datetime object to string object just for presentation purposes - here you can specify any format you like\n",
    "    df_asb['Formatted Date'] = df_asb['Date'].apply(lambda x: x.strftime('%d-%m-%Y-%A'))\n",
    "    \n",
    "    # reorder columns\n",
    "    df_asb = df_asb[['Date','Formatted Date','Description','Payee','Memo','Amount','File']]\n",
    "    \n",
    "    # append each dataframe to a list\n",
    "    df_list_asb.append(df_asb)\n",
    "   \n",
    "#df_list\n",
    "\n",
    "final_df_asb = pd.concat(df_list_asb)\n",
    "final_df_asb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9407b38-1816-452b-ab21-b6bcc3b123e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_asb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e54783-4d65-413f-b87b-0ea283a22f39",
   "metadata": {},
   "source": [
    "### load ASB data to \"Processed-Silver\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2e90b-577e-44e0-82ee-a9b1a1e25f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_asb_procs = Path(r'../Data/2.Processed-Silver/')\n",
    "file_asb_processed = folder_path_asb_procs / \"ASB_Transactions.csv\"\n",
    "final_df_asb.to_csv(file_asb_processed, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43a047",
   "metadata": {},
   "source": [
    "# extract and transform QCard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf55a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_Qcard = Path(r'../Data/1.Raw-Bronze/QCard/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd248fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(folder_path_Qcard.rglob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_Qcard = []\n",
    "for file in folder_path_Qcard.rglob('*.csv'):\n",
    "    df_Q = pd.read_csv(file, usecols=['Date','Description','Amount'])[['Date','Description','Amount']]\n",
    "    \n",
    "    # adding a new 'File' column to df with file name\n",
    "    df_Q['File'] = file.stem\n",
    "\n",
    "    # convert to lower case\n",
    "    columns_to_convert = ['Description']\n",
    "    df_Q[columns_to_convert] = df_Q[columns_to_convert].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # Convert 'Amount' column to float - had to replace $ and , with empty char\n",
    "    df_Q['Amount'] = df_Q['Amount'].apply(lambda x: float(x.replace('$', '').replace(',', '')))\n",
    "\n",
    "    # convert Date type object to date datatype\n",
    "    # - step 1 - convert series of dates in original 'Date' column from string to datetime object (this is needed for date calculations)\n",
    "    df_Q['Date'] = pd.to_datetime(df_Q['Date'], dayfirst=True)\n",
    "    # - step 2 - re-convert 'Date' datetime column values from datetime object to string object just for presentation purposes - here you can specify any format you like\n",
    "    df_Q['Formatted Date'] = df_Q['Date'].apply(lambda x: x.strftime('%d-%m-%Y-%A'))\n",
    "    \n",
    "    # reorder columns\n",
    "    df_Q = df_Q[['Date','Formatted Date','Description','Amount','File']]\n",
    "    \n",
    "    # append each dataframe to a list\n",
    "    df_list_Qcard.append(df_Q)\n",
    "   \n",
    "#df_list\n",
    "\n",
    "final_df_Q = pd.concat(df_list_Qcard)\n",
    "final_df_Q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a626cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_Q.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb1cbc",
   "metadata": {},
   "source": [
    "### load QCard data to \"Processed-Silver\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_Qcard_procs = Path(r'../Data/2.Processed-Silver/')\n",
    "file_Qcard_processed = folder_path_Qcard_procs / \"QCard_Transactions.csv\"\n",
    "final_df_Q.to_csv(file_Qcard_processed, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884114d",
   "metadata": {},
   "source": [
    "# extract and transform ASB CC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_asb_cc = Path(r'../Data/1.Raw-Bronze/ASBCC/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b179d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(folder_path_asb_cc.rglob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_asb_cc = []\n",
    "for file in folder_path_asb_cc.rglob('*.csv'):\n",
    "    df_asb_cc = pd.read_csv(file, usecols=['Date of Transaction','Description','Amount'])\n",
    "    df_asb_cc = df_asb_cc.rename(columns={'Date of Transaction':'Date'})\n",
    "    \n",
    "    # adding a new 'File' column to df with file name\n",
    "    df_asb_cc = df_asb_cc.dropna()\n",
    "    df_asb_cc['File'] = file.stem\n",
    "\n",
    "    # convert to lower case\n",
    "    columns_to_convert = ['Description']\n",
    "    df_asb_cc[columns_to_convert] = df_asb_cc[columns_to_convert].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # Convert 'Amount' column to float\n",
    "    df_asb_cc['Amount'] = df_asb_cc['Amount'].apply(lambda x: float(x))\n",
    "\n",
    "    # CC Amount column has different format than other files - so here I'm subtracting each value by 0 to swap the - & + values\n",
    "    df_asb_cc['Amount'] = df_asb_cc['Amount'].apply(lambda x: (0-x))\n",
    "\n",
    "\n",
    "    # convert Date type object to date datatype\n",
    "    # - step 1 - convert series of dates in original 'Date' column from string to datetime object (this is needed for date calculations)\n",
    "    df_asb_cc['Date'] = pd.to_datetime(df_asb_cc['Date'], dayfirst=True)\n",
    "    # - step 2 - re-convert 'Date' datetime column values from datetime object to string object just for presentation purposes - here you can specify any format you like\n",
    "    # df_asb_cc['Formatted_Date'] = df_asb_cc['Date'].apply(lambda x: x.strftime('%d-%m-%Y-%A'))\n",
    "    \n",
    "    # reorder columns\n",
    "    df_asb_cc = df_asb_cc[['Date','Description','Amount','File']]\n",
    "    \n",
    "    # append each dataframe to a list\n",
    "    df_list_asb_cc.append(df_asb_cc)\n",
    "   \n",
    "#df_list\n",
    "\n",
    "final_asb_cc = pd.concat(df_list_asb_cc)\n",
    "final_asb_cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1697c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_asb_cc.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ded1f",
   "metadata": {},
   "source": [
    "### load ASB CC data to \"Processed-Silver\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc040030",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_asb_cc_procs = Path(r'../Data/2.Processed-Silver/')\n",
    "file_asb_cc_processed = folder_path_asb_cc_procs / \"ASB_CC_Transactions.csv\"\n",
    "final_asb_cc.to_csv(file_asb_cc_processed, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537b517",
   "metadata": {},
   "source": [
    "# append all files in processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040aebab-eac1-414c-aa65-0f14511d738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_processed = Path(r'../Data/2.Processed-Silver/')\n",
    "list(folder_path_processed.rglob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6aa05c-0d46-4b50-a870-ba7be742f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_processed = []\n",
    "for file in folder_path_processed.rglob('*.csv'):\n",
    "    df_processed = pd.read_csv(file, usecols=['Date','Description','Amount','File'])[['Date','Description','Amount','File']]\n",
    "        \n",
    "    # append each dataframe to a list\n",
    "    df_list_processed.append(df_processed)\n",
    "   \n",
    "final_df_processed = pd.concat(df_list_processed)\n",
    "\n",
    "# fill missing values: replace all NaN (not a number) values in the 'final_df_processed' dataframe with 'N/A'.\n",
    "final_df_processed.fillna('N/A', inplace=True)\n",
    "\n",
    "# convert date format: using a lambda function, change the 'date' column in the 'final_df_processed' dataframe, from the string format \"yyyy-mm-dd\" to a datetime object.\n",
    "final_df_processed['Date'] = final_df_processed['Date'].apply(lambda x:datetime.strptime(x, \"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cce304",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the number of rows and columns in the 'final_df_processed' DataFrame.\n",
    "final_df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the 'sub category' column in the 'final_df_processed' DataFrame to have the value 'unassigned' for all rows.\n",
    "final_df_processed['Sub Category'] = 'Unassigned'\n",
    "final_df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282fea1",
   "metadata": {},
   "source": [
    "# experimenting automating sub categories with fuzzy matching - dynamically create sub categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd671606",
   "metadata": {},
   "source": [
    "#### automation process for categorizing transactions based on their descriptions, using fuzzy matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d080c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for string matching and similarity comparison\n",
    "from thefuzz import fuzz, process\n",
    "\n",
    "# convert the values from the 'Description' column of the 'final_df_processed' DataFrame to a list.\n",
    "discription_list = final_df_processed['Description'].tolist()\n",
    "\n",
    "# list of categories to add to the 'Sub Category' column\n",
    "my_list = ['barfoot','auto finance','gas','mylotto','transfer','cake','restaurant','baby','pizza','ez wash','laundry','dash','beenz','rayden','big chill', 'paramount','transcoint','producepronto','uber','kfc','pak n save','countdown','new world','mcdonald','noodle canteen','menulog','dsouza']\n",
    "\n",
    "for item in my_list:\n",
    "    # fuzzy matching: using the 'process.extractOne' function, find the closest matching item to the given 'item' from the 'description_list', and store the result in the 'match_list'\n",
    "    match_list = list(process.extractOne(item, discription_list))\n",
    "\n",
    "    # same fuzz function, but with 'limit' you can have number of checks with different matching score\n",
    "    # match_list = list(process.extract(item, discription_list, limit=3))\n",
    "\n",
    "    # using 'scorer=fuzz.token_sort_ratio' method\n",
    "    # match_list = list(process.extract(item, discription_list, limit=8, scorer=fuzz.token_sort_ratio))\n",
    "\n",
    "    # fuzz gives a list of matching values and matching scores\n",
    "    print(item,'=', match_list)\n",
    "\n",
    "    # if the matching score between the keyword and the best match is greater than or equal to 80 (as determined by match_list[1]), the code enters a conditional block.\n",
    "    if int(match_list[1]) >= 80:\n",
    "        final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(item), str(item).title(), final_df_processed['Sub Category'])\n",
    "\n",
    "final_df_processed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7542cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and view: import the 'xlwings' library and use it to display the 'final_df_processed' DataFrame in Excel\n",
    "import xlwings as xw\n",
    "xw.view(final_df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60bfb7",
   "metadata": {},
   "source": [
    "# assign sub-categories - a manual way of sub categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------------------------------------------------\n",
    "salary = 'beenz|rayden|big chill|paramount|transcoint|producepronto'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(salary) & (final_df_processed['Amount'] > 0), 'Salary', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "other_income = 'dsouza|upwork|freelance|trading'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(other_income) & (final_df_processed['Amount'] > 0), 'Other Income', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "rental = 'barfootnthomp|raywhite|kiwiprop'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(rental) & (final_df_processed['Amount'] < 0), 'Rental', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "utility = 'water|carpetclean|matagas|pulse energy|kiwi elec|watercare'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(utility) & (final_df_processed['Amount'] < 0), 'Utility Bills', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "restr = 'hambagu|donuts|zoo keeper|burger|bakery|pavilion|mac roast|calimero|bakeries|wendys|starbucks|mcdonald|smovenpick|kitchen|lunch bar|takeaway|subway|cuisine|pizza|bunga raya|umiya|father ted|kfc|aroy|lonestar|burgerfuel|cake|restaurant|paradise|top in town|wok|takashi|kreem|scoopers|mamarich|cafe'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(restr) & (final_df_processed['Amount'] < 0), 'Restaurants', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "grocs = 'countdown|pak n save|serandibmt|spiceinvader'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(grocs) & (final_df_processed['Amount'] < 0), 'Groceries', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "transfers = 'transfer|transfers'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(transfers), 'Transfers', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "transport = 'carjam|nz transport agency|nz motor factory|vtnz'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(transport) & (final_df_processed['Amount'] < 0), 'Motor Vehicle', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "fuel = 'fuel|filling|mobil|waitomo'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(fuel) & (final_df_processed['Amount'] < 0), 'Fuel', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "travel = 'qantas|transco|uber|dash|athop|airnz'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(travel) & (final_df_processed['Amount'] < 0), 'Travel', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "internet = 'slingshot'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(internet) & (final_df_processed['Amount'] < 0), 'Internet', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "gym = 'jetts|cityfitness'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(gym) & (final_df_processed['Amount'] < 0), 'Gym', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "Laundry = 'laundromat|laundry'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(Laundry) & (final_df_processed['Amount'] < 0), 'Laundry', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "entertainment = 'hoyts|event cinemas'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(entertainment) & (final_df_processed['Amount'] < 0), 'Entertainment', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "car_finance = 'auto finance direct|toyota finance'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(car_finance) & (final_df_processed['Amount'] < 0), 'Car Finanace', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "insurance = 'aia nz|aioi nissay'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(insurance) & (final_df_processed['Amount'] < 0), 'Insurance', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "shopping = 'bunnings|paper plus|briscoes|shein|aliexpress|baby fac|kmart|pb technologies|cotton on'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(shopping) & (final_df_processed['Amount'] < 0), 'Shopping', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "telecom = '2degrees|vodafone|onenz'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(telecom) & (final_df_processed['Amount'] < 0), 'Telecom', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "edu = 'industryconnec|industry connect|udemy'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(edu) & (final_df_processed['Amount'] < 0), 'Education', final_df_processed['Sub Category'])\n",
    "#---------------------------------------------------------\n",
    "health = 'otahuhu health|local doctors'\n",
    "final_df_processed['Sub Category'] = np.where(final_df_processed['Description'].str.contains(health) & (final_df_processed['Amount'] < 0), 'Health & Medical', final_df_processed['Sub Category'])\n",
    "\n",
    "\n",
    "\n",
    "final_df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eae48f",
   "metadata": {},
   "source": [
    "# assign categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5da85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column \"Category\" and add 'Unassigned' to each row initially\n",
    "final_df_processed['Category'] = 'Unassigned'\n",
    "\n",
    "#---------------------------------------------------------\n",
    "income = 'Other Income|Salary'\n",
    "final_df_processed['Category'] = np.where(final_df_processed['Sub Category'].str.contains(income), 'Earnings', final_df_processed['Category'])\n",
    "\n",
    "#---------------------------------------------------------\n",
    "transport = 'Fuel|Car Finance|Motor Vehicle'\n",
    "final_df_processed['Category'] = np.where(final_df_processed['Sub Category'].str.contains(transport), 'Transport', final_df_processed['Category'])\n",
    "\n",
    "#---------------------------------------------------------\n",
    "living = 'Groceries|Restaurants|Shopping'\n",
    "final_df_processed['Category'] = np.where(final_df_processed['Sub Category'].str.contains(living), 'Living Expenses', final_df_processed['Category'])\n",
    "\n",
    "\n",
    "final_df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba65b1",
   "metadata": {},
   "source": [
    "# assign category type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_processed['Category Type'] = 'Unassigned'\n",
    "\n",
    "#---------------------------------------------------------\n",
    "income_type = 'Earnings'\n",
    "final_df_processed['Category Type'] = np.where(final_df_processed['Category'].str.contains(income_type), 'Income', final_df_processed['Category Type'])\n",
    "\n",
    "#---------------------------------------------------------\n",
    "expense_type = 'Transport|Living Expenses'\n",
    "final_df_processed['Category Type'] = np.where(final_df_processed['Category'].str.contains(expense_type), 'Expense', final_df_processed['Category Type'])\n",
    "\n",
    "final_df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f6772",
   "metadata": {},
   "source": [
    "# exporting data to master File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12af6b4-05b5-4142-a215-57c9f79a7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_presentation = Path(r'../Data/3.Presentation-Gold/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927a041",
   "metadata": {},
   "source": [
    "#### select date range to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f75627",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(\"2023-01-01\")\n",
    "end_date = pd.to_datetime(\"2023-06-30\")\n",
    "\n",
    "final_df_processed_ranged = final_df_processed[(final_df_processed['Date'] >= start_date) & (final_df_processed['Date'] <= end_date)]\n",
    "\n",
    "# export data from above given date range\n",
    "file_master = folder_path_presentation / \"Master_File.csv\"\n",
    "final_df_processed_ranged.to_csv(file_master, index=False)\n",
    "final_df_processed_ranged.head()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# same with use of a function\n",
    "# def select_date_range(df:final_df_processed, start, end)->pd.DataFrame:\n",
    "#     final_df_processed_ranged = df[(df['Date'] >= pd.to_datetime(start)) & (df['Date'] <= pd.to_datetime(end))]\n",
    "#     return final_df_processed_ranged\n",
    "\n",
    "\n",
    "# Calling the function\n",
    "# option to set a date range for export\n",
    "# fdf = select_date_range(final_df_processed,\"2023-01-01\", \"2023-06-30\")\n",
    "# file_master = folder_path_presentation / \"Master_File.csv\"\n",
    "# fdf.to_csv(file_master, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581c639",
   "metadata": {},
   "source": [
    "#### export the full final dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_master = folder_path_presentation / \"Master_File.csv\"\n",
    "final_df_processed.to_csv(file_master, index=False)\n",
    "final_df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab5018",
   "metadata": {},
   "source": [
    "# show unassigned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeddbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unassigned = final_df_processed.loc[final_df_processed['Category'] =='Unassigned']\n",
    "# unassigned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dd1b8",
   "metadata": {},
   "source": [
    "# optional step - try later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351c1c3-a0bc-4734-ae27-4da0ec490022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path_to_check = Path(r'C:\\Users\\sular\\OneDrive\\1.Data Analysis & Engineering\\Data Engineering\\Data Engineering Projects\\12.Personal Finance Dashboard - Power BI\\Data\\2.Processed-Silver')\n",
    "# list_A = list(file_path_to_check.rglob('*.csv'))\n",
    "# list_A\n",
    "\n",
    "# for file in list_A:\n",
    "#     if os.path.isfile(file):\n",
    "#         os.remove(file)\n",
    "#     else:\n",
    "#         final_df_asb.to_csv(file_path_asb, index=False)\n",
    "#         final_df_anz.to_csv(file_path_anz, index=False)\n",
    "\n",
    "\n",
    "# if list_A=0:\n",
    "#     print(f\"File does not exist. New file is downloaded.\")\n",
    "#     final_df_asb.to_csv(file_path_asb, index=False)\n",
    "#     final_df_anz.to_csv(file_path_anz, index=False)\n",
    "# else:\n",
    "#     os.remove(file_path)\n",
    "#     print(f\"File is already exists and its deleted. Please run the code again\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5c543",
   "metadata": {},
   "source": [
    "# loading data to SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlalchemy, pyodbc, socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642921da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyodbc.drivers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbd8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlalchemy.create_engine(f'mssql+pyodbc://{socket.gethostname()}/Personal_Finanace?trusted_connection=yes&driver=ODBC Driver 17 for SQL Server')\n",
    "# final_df_processed.to_sql(\"Master_Data\", con=conn, if_exists=\"replace\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
